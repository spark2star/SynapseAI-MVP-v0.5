"""
Simple REST API for audio transcription with Mental Health Context + Voice Activity Detection.
Optimized for Marathi-Hindi code-mixed therapy sessions with doctor-only voice filtering.
"""

from fastapi import APIRouter, Depends, HTTPException, File, UploadFile, Query
from sqlalchemy.orm import Session
from google.cloud import speech_v2 as speech
from google.cloud.speech_v2.types import cloud_speech
from google.api_core.exceptions import GoogleAPIError
import logging
import io
import tempfile
import os
import subprocess
import wave
import numpy as np

from app.core.database import get_db
from app.core.dependencies import get_current_user
from app.models.user import User
from app.core.config import settings

router = APIRouter()
logger = logging.getLogger(__name__)

# ===========================
# ðŸ§  MENTAL HEALTH CONTEXT
# ===========================
MENTAL_HEALTH_PHRASES = [
    # === CORE MENTAL HEALTH TERMS ===
    # Hindi
    {"value": "à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤¸à¥à¤µà¤¾à¤¸à¥à¤¥à¥à¤¯", "boost": 15},
    {"value": "à¤…à¤µà¤¸à¤¾à¤¦", "boost": 15},
    {"value": "à¤šà¤¿à¤‚à¤¤à¤¾", "boost": 15},
    {"value": "à¤¤à¤¨à¤¾à¤µ", "boost": 15},
    {"value": "à¤¡à¤¿à¤ªà¥à¤°à¥‡à¤¶à¤¨", "boost": 15},
    {"value": "à¤®à¤¨à¥‹à¤šà¤¿à¤•à¤¿à¤¤à¥à¤¸à¤•", "boost": 15},
    {"value": "à¤®à¤¨à¥‹à¤µà¥ˆà¤œà¥à¤žà¤¾à¤¨à¤¿à¤•", "boost": 15},
    
    # Marathi
    {"value": "à¤®à¤¾à¤¨à¤¸à¤¿à¤• à¤†à¤°à¥‹à¤—à¥à¤¯", "boost": 15},
    {"value": "à¤¨à¥ˆà¤°à¤¾à¤¶à¥à¤¯", "boost": 15},
    {"value": "à¤¤à¤¾à¤£", "boost": 15},
    {"value": "à¤®à¤¾à¤¨à¤¸à¥‹à¤ªà¤šà¤¾à¤° à¤¤à¤œà¥à¤œà¥à¤ž", "boost": 15},
    {"value": "à¤®à¤¾à¤¨à¤¸à¤¶à¤¾à¤¸à¥à¤¤à¥à¤°à¤œà¥à¤ž", "boost": 15},
    
    # === CODE-MIXED COMMON PHRASES ===
    {"value": "depression à¤†à¤¹à¥‡", "boost": 14},
    {"value": "depression à¤¹à¥ˆ", "boost": 14},
    {"value": "anxiety à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 14},
    {"value": "anxiety à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ", "boost": 14},
    {"value": "stress à¤¹à¥‹à¤¤à¥‹", "boost": 14},
    {"value": "stress à¤†à¤¹à¥‡", "boost": 14},
    {"value": "panic attack", "boost": 14},
    {"value": "panic attack à¤†à¤²à¤¾", "boost": 12},
    {"value": "panic attack à¤¹à¥à¤†", "boost": 12},
    
    # === THERAPY RELATED ===
    {"value": "therapy à¤˜à¥‡à¤¤à¥‹", "boost": 12},
    {"value": "therapy à¤²à¥‡ à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚", "boost": 12},
    {"value": "counseling à¤•à¥‡à¤²à¥€", "boost": 12},
    {"value": "counseling à¤²à¥€", "boost": 12},
    {"value": "medication à¤šà¤¾à¤²à¥‚ à¤†à¤¹à¥‡", "boost": 12},
    {"value": "medication à¤²à¥‡ à¤°à¤¹à¤¾ à¤¹à¥‚à¤‚", "boost": 12},
    {"value": "psychiatrist à¤•à¤¡à¥‡ à¤œà¤¾à¤¤à¥‹", "boost": 12},
    {"value": "psychiatrist à¤•à¥‡ à¤ªà¤¾à¤¸ à¤œà¤¾à¤¤à¤¾ à¤¹à¥‚à¤‚", "boost": 12},
    {"value": "psychologist à¤•à¤¡à¥‡ à¤—à¥‡à¤²à¥‹", "boost": 12},
    
    # === SYMPTOMS - HINDI ===
    {"value": "à¤¨à¥€à¤‚à¤¦ à¤¨à¤¹à¥€à¤‚ à¤†à¤¤à¥€", "boost": 13},
    {"value": "à¤­à¥‚à¤– à¤¨à¤¹à¥€à¤‚ à¤²à¤—à¤¤à¥€", "boost": 13},
    {"value": "à¤¦à¤¿à¤² à¤˜à¤¬à¤°à¤¾à¤¤à¤¾ à¤¹à¥ˆ", "boost": 13},
    {"value": "à¤°à¥‹à¤¨à¤¾ à¤†à¤¤à¤¾ à¤¹à¥ˆ", "boost": 13},
    {"value": "à¤¥à¤•à¤¾à¤¨ à¤®à¤¹à¤¸à¥‚à¤¸ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ", "boost": 12},
    {"value": "à¤§à¥à¤¯à¤¾à¤¨ à¤¨à¤¹à¥€à¤‚ à¤²à¤—à¤¤à¤¾", "boost": 12},
    {"value": "à¤—à¥à¤¸à¥à¤¸à¤¾ à¤†à¤¤à¤¾ à¤¹à¥ˆ", "boost": 12},
    {"value": "à¤…à¤•à¥‡à¤²à¤¾ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤‚", "boost": 12},
    
    # === SYMPTOMS - MARATHI ===
    {"value": "à¤à¥‹à¤ª à¤¯à¥‡à¤¤ à¤¨à¤¾à¤¹à¥€", "boost": 13},
    {"value": "à¤­à¥‚à¤• à¤²à¤¾à¤—à¤¤ à¤¨à¤¾à¤¹à¥€", "boost": 13},
    {"value": "à¤¹à¥ƒà¤¦à¤¯ à¤§à¤¡à¤§à¤¡à¤¤à¥‡", "boost": 13},
    {"value": "à¤°à¤¡à¥‚ à¤¯à¥‡à¤¤à¥‡", "boost": 13},
    {"value": "à¤¥à¤•à¤µà¤¾ à¤œà¤¾à¤£à¤µà¤¤à¥‹", "boost": 12},
    {"value": "à¤²à¤•à¥à¤· à¤²à¤¾à¤—à¤¤ à¤¨à¤¾à¤¹à¥€", "boost": 12},
    {"value": "à¤°à¤¾à¤— à¤¯à¥‡à¤¤à¥‹", "boost": 12},
    {"value": "à¤à¤•à¤Ÿà¥‡ à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 12},
    
    # === FEELINGS - HINDI ===
    {"value": "à¤®à¥à¤à¥‡ à¤²à¤—à¤¤à¤¾ à¤¹à¥ˆ", "boost": 11},
    {"value": "à¤®à¥ˆà¤‚ à¤¸à¤®à¤à¤¤à¤¾ à¤¹à¥‚à¤‚", "boost": 11},
    {"value": "à¤®à¥ˆà¤‚ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤°à¤¤à¤¾ à¤¹à¥‚à¤‚", "boost": 11},
    {"value": "à¤®à¥à¤à¥‡ à¤¡à¤° à¤²à¤—à¤¤à¤¾ à¤¹à¥ˆ", "boost": 11},
    {"value": "à¤®à¥à¤à¥‡ à¤šà¤¿à¤‚à¤¤à¤¾ à¤¹à¥‹à¤¤à¥€ à¤¹à¥ˆ", "boost": 11},
    
    # === FEELINGS - MARATHI ===
    {"value": "à¤®à¤²à¤¾ à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 11},
    {"value": "à¤®à¥€ à¤¸à¤®à¤œà¤¤à¥‹", "boost": 11},
    {"value": "à¤®à¤²à¤¾ à¤œà¤¾à¤£à¤µà¤¤à¥‡", "boost": 11},
    {"value": "à¤®à¤²à¤¾ à¤­à¥€à¤¤à¥€ à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 11},
    {"value": "à¤®à¤²à¤¾ à¤•à¤¾à¤³à¤œà¥€ à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 11},
    
    # === COMMON THERAPY GREETINGS - HINDI ===
    {"value": "à¤•à¥ˆà¤¸à¥‡ à¤¹à¥ˆà¤‚ à¤†à¤ª", "boost": 10},
    {"value": "à¤†à¤ª à¤•à¥ˆà¤¸à¤¾ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤° à¤°à¤¹à¥‡ à¤¹à¥ˆà¤‚", "boost": 10},
    {"value": "à¤•à¥à¤¯à¤¾ à¤¹à¥à¤†", "boost": 10},
    {"value": "à¤¬à¤¤à¤¾à¤‡à¤", "boost": 10},
    
    # === COMMON THERAPY GREETINGS - MARATHI ===
    {"value": "à¤•à¤¸à¥‡ à¤†à¤¹à¤¾à¤¤", "boost": 10},
    {"value": "à¤¤à¥à¤®à¥à¤¹à¤¾à¤²à¤¾ à¤•à¤¸à¥‡ à¤µà¤¾à¤Ÿà¤¤à¥‡", "boost": 10},
    {"value": "à¤•à¤¾à¤¯ à¤à¤¾à¤²à¥‡", "boost": 10},
    {"value": "à¤¸à¤¾à¤‚à¤—à¤¾", "boost": 10},
    
    # === MEDICATION NAMES ===
    {"value": "antidepressant", "boost": 12},
    {"value": "anxiolytic", "boost": 12},
    {"value": "sleeping pill", "boost": 11},
    {"value": "tablet à¤˜à¥‡à¤¤à¥‹", "boost": 11},
    {"value": "medicine à¤˜à¥‡à¤¤à¤²à¥€", "boost": 11},
]


# ===========================
# ðŸŽ¤ VOICE ACTIVITY DETECTION
# ===========================
def is_doctor_voice(audio_data: bytes) -> tuple[bool, dict]:
    """
    Verify audio is from doctor (collar mic) based on amplitude.
    Doctor's voice should be consistently HIGH amplitude.
    
    Returns:
        tuple: (is_valid, stats_dict)
    """
    try:
        # Read WAV data
        with wave.open(io.BytesIO(audio_data), 'rb') as wav:
            frames = wav.readframes(wav.getnframes())
            audio_array = np.frombuffer(frames, dtype=np.int16)
            duration = wav.getnframes() / wav.getframerate()
        
        # Calculate audio statistics
        max_amp = int(np.max(np.abs(audio_array)))
        avg_amp = float(np.mean(np.abs(audio_array)))
        
        stats = {
            "max_amplitude": max_amp,
            "avg_amplitude": round(avg_amp, 1),
            "duration": round(duration, 2)
        }
        
        logger.info(f"[VAD] ðŸ“Š Audio stats: max={max_amp}, avg={avg_amp:.1f}, duration={duration:.2f}s")
        
        # âœ… LOWERED THRESHOLDS - Less aggressive filtering to capture all speech
        # - Max amplitude threshold: 1000 (was 5000)
        # - Average amplitude threshold: 300 (was 1000)
        MIN_DOCTOR_MAX_AMP = 1000  # Lowered from 5000
        MIN_DOCTOR_AVG_AMP = 300   # Lowered from 1000
        
        if max_amp < MIN_DOCTOR_MAX_AMP:
            logger.warning(f"[VAD] ðŸ”‡ Max amplitude too low ({max_amp}) - likely complete silence")
            return False, stats
        
        if avg_amp < MIN_DOCTOR_AVG_AMP:
            logger.warning(f"[VAD] ðŸ”‡ Avg amplitude too low ({avg_amp:.1f}) - likely background noise only")
            # âœ… CHANGED: Still send to transcription, let STT decide
            logger.info("[VAD] âš ï¸ Low amplitude but sending anyway for STT analysis")
        
        logger.info("[VAD] âœ… Audio verified as doctor's voice")
        return True, stats
        
    except Exception as e:
        logger.error(f"[VAD] âŒ Error checking audio amplitude: {str(e)}")
        # On error, assume it's valid and let transcription handle it
        return True, {"error": str(e)}


@router.post("/chunk")
async def transcribe_audio_chunk(
    session_id: str = Query(...),
    audio: UploadFile = File(...),
    db: Session = Depends(get_db),
    current_user: User = Depends(get_current_user)
):
    """
    Transcribe a single audio chunk (5-10 seconds) with mental health context.
    Uses Speech Adaptation for better recognition of Marathi-Hindi code-mixed therapy sessions.
    Filters out patient voice using VAD amplitude detection.
    """
    import traceback
    import sys
    
    # Forces output to terminal
    print("\n" + "="*80, file=sys.stderr)
    print("ðŸŽ¯ [MENTAL HEALTH STT] transcribe_simple.py CALLED!", file=sys.stderr)
    print("="*80 + "\n", file=sys.stderr)
    
    logger.info(f"ðŸŽ¯ [STT] Starting transcription for session: {session_id}")
    logger.info(f"ðŸ‘¤ [STT] User ID: {current_user.id}")
    logger.info(f"ðŸ“‚ [STT] Audio file: {audio.filename}, content_type: {audio.content_type}")
    
    wav_path = None
    temp_audio_path = None
    
    try:
        logger.info(f"ðŸ“¥ [STT] Step 1: Reading audio data...")
        audio_content = await audio.read()
        logger.info(f"âœ… [STT] Step 1 complete: Audio read successfully - {len(audio_content)} bytes")
        
        if len(audio_content) == 0:
            logger.warning("âš ï¸ [STT] Empty audio chunk received")
            return {
                "status": "success",
                "transcript": "",
                "confidence": 0.0,
                "session_id": session_id,
                "message": "Empty audio chunk"
            }
        
        # Initialize Speech client
        logger.info("ðŸ”Œ [STT] Step 2: Initializing Google Speech client...")
        client = speech.SpeechClient()
        logger.info("âœ… [STT] Step 2 complete: Speech client initialized")
        
        # Save WEBM to temporary file for conversion
        logger.info("ðŸ”„ [STT] Step 2.5: Saving audio to temp file for processing...")
        with tempfile.NamedTemporaryFile(suffix=".webm", delete=False) as temp_audio:
            temp_audio.write(audio_content)
            temp_audio_path = temp_audio.name
        logger.info(f"âœ… [STT] Audio saved to: {temp_audio_path}")
        
        try:
            # Convert WEBM to WAV using ffmpeg
            logger.info("ðŸ”„ [STT] Step 2.6: Converting WEBM to WAV...")
            wav_path = temp_audio_path.replace(".webm", ".wav")
            
            subprocess.run([
                "ffmpeg", "-i", temp_audio_path,
                "-acodec", "pcm_s16le",
                "-ar", "16000",
                "-ac", "1",
                wav_path
            ], check=True, capture_output=True)
            
            logger.info(f"âœ… [STT] Conversion complete: {wav_path}")
            
            # Read converted WAV
            with open(wav_path, "rb") as f:
                audio_content = f.read()
                logger.info(f"âœ… [STT] WAV file read: {len(audio_content)} bytes")
            
            # ===========================
            # ðŸŽ¤ STEP 3: VAD AMPLITUDE CHECK
            # ===========================
            logger.info("ðŸŽ¤ [VAD] Step 3: Checking if audio is from doctor (collar mic)...")
            is_valid, audio_stats = is_doctor_voice(audio_content)
            
            if not is_valid:
                logger.info("[VAD] ðŸš« Audio rejected - not doctor's voice (patient or background noise)")
                # Clean up temp files
                os.unlink(temp_audio_path)
                os.unlink(wav_path)
                
                return {
                    "status": "filtered",
                    "transcript": "",  # Return empty transcript
                    "confidence": 0.0,
                    "language_detected": "mr-IN",
                    "session_id": session_id,
                    "context": "mental_health",
                    "message": "Audio filtered - patient voice detected",
                    "audio_stats": audio_stats
                }
            
            logger.info(f"âœ… [VAD] Step 3 complete: Audio verified as doctor's voice")
            
            # Clean up temp files (keep WAV content in memory)
            os.unlink(temp_audio_path)
            os.unlink(wav_path)
            
        except FileNotFoundError:
            logger.error("âŒ [STT] ffmpeg not found! Install with: brew install ffmpeg")
            if temp_audio_path and os.path.exists(temp_audio_path):
                os.unlink(temp_audio_path)
            raise HTTPException(
                status_code=500,
                detail="Audio conversion failed: ffmpeg not installed"
            )
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ [STT] ffmpeg conversion failed: {e.stderr.decode()}")
            if temp_audio_path and os.path.exists(temp_audio_path):
                os.unlink(temp_audio_path)
            if wav_path and os.path.exists(wav_path):
                os.unlink(wav_path)
            raise HTTPException(
                status_code=500,
                detail=f"Audio conversion failed: {e.stderr.decode()}"
            )
        
        # ===========================
        # ðŸ§  CONFIGURE RECOGNITION WITH MENTAL HEALTH CONTEXT
        # ===========================
        logger.info("âš™ï¸ [STT] Step 4: Creating recognition config with mental health context...")
        
        # Convert phrase list to PhraseSet format
        phrase_set = cloud_speech.PhraseSet()
        for phrase_data in MENTAL_HEALTH_PHRASES:
            phrase_set.phrases.append(
                cloud_speech.PhraseSet.Phrase(
                    value=phrase_data["value"],
                    boost=phrase_data["boost"]
                )
            )
        
        config = speech.RecognitionConfig(
            explicit_decoding_config=speech.ExplicitDecodingConfig(
                encoding=speech.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                audio_channel_count=1,
            ),
            model="latest_short",  # âœ… Optimized for 30-second chunks
            language_codes=["mr-IN", "hi-IN", "en-IN"],  # Marathi, Hindi, English
            features=cloud_speech.RecognitionFeatures(
                enable_word_time_offsets=True,
                enable_word_confidence=True,
            ),
            # âœ… SPEECH ADAPTATION FOR MENTAL HEALTH TERMINOLOGY
            adaptation=cloud_speech.SpeechAdaptation(
                phrase_sets=[
                    cloud_speech.SpeechAdaptation.AdaptationPhraseSet(
                        inline_phrase_set=phrase_set
                    )
                ]
            )
        )
        
        logger.info(f"âœ… [STT] Step 4 complete: Config created with mental health context")
        logger.info(f"   - Model: latest_short")
        logger.info(f"   - Languages: mr-IN, hi-IN, en-IN")
        logger.info(f"   - Mental health phrases loaded: {len(MENTAL_HEALTH_PHRASES)}")
        
        # Create recognition request
        logger.info(f"ðŸ“‹ [STT] Step 5: Creating recognition request...")
        recognizer_path = f"projects/{settings.GOOGLE_CLOUD_PROJECT}/locations/{settings.VERTEX_AI_LOCATION}/recognizers/_"
        logger.info(f"ðŸŽ¯ [STT] Recognizer path: {recognizer_path}")
        
        request = speech.RecognizeRequest(
            recognizer=recognizer_path,
            config=config,
            content=audio_content
        )
        logger.info(f"âœ… [STT] Step 5 complete: Request object created")
        
        logger.info(f"ðŸš€ [STT] Step 6: Calling Google Speech API with mental health context...")

        # Transcribe with timeout
        try:
            response = client.recognize(
                request=request,
                timeout=30.0
            )
            logger.info(f"âœ… [STT] Step 6 complete: Got response from Google Speech API")
        except Exception as api_error:
            logger.error(f"âŒ [STT] Google Speech API call failed: {str(api_error)}")
            logger.error(f"ðŸ“‹ [STT] Error type: {type(api_error).__name__}")
            raise

        # Extract transcript
        logger.info(f"ðŸ“ [STT] Step 7: Extracting transcript from response...")
        transcript = ""
        confidence = 0.0
        language_detected = "unknown"
        
        if response.results:
            logger.info(f"ðŸ“Š [STT] Got {len(response.results)} results from Speech API")
            for idx, result in enumerate(response.results):
                if result.alternatives:
                    alt = result.alternatives[0]
                    transcript += alt.transcript + " "
                    if hasattr(alt, 'confidence'):
                        confidence = max(confidence, alt.confidence)
                    
                    if hasattr(result, 'language_code'):
                        language_detected = result.language_code
                    
                    logger.info(f"  Result {idx+1}: '{alt.transcript}' (confidence: {alt.confidence if hasattr(alt, 'confidence') else 'N/A'})")
        else:
            logger.warning("âš ï¸ [STT] No results from Speech API (silence detected)")
        
        transcript = transcript.strip()
        logger.info(f"âœ… [STT] Step 7 complete: Transcription extracted")
        logger.info(f"ðŸ“„ [STT] Final transcript: '{transcript[:100]}...' (length: {len(transcript)}, confidence: {confidence:.2f})")
        logger.info(f"ðŸŒ [STT] Detected language: {language_detected}")
        
        logger.info(f"ðŸŽ‰ [STT] Step 8: Returning successful response")
        return {
            "status": "success",
            "transcript": transcript,
            "confidence": confidence,
            "language_detected": language_detected,
            "session_id": session_id,
            "context": "mental_health",
            "audio_stats": audio_stats
        }
        
    except GoogleAPIError as e:
        error_traceback = traceback.format_exc()
        logger.error(f"âŒ [STT] GOOGLE API ERROR: {str(e)}")
        logger.error(f"ðŸ“‹ [STT] Error type: {type(e).__name__}")
        logger.error(f"ðŸ“‹ [STT] Full traceback:\n{error_traceback}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "google_api_error",
                "message": f"Google Speech API error: {str(e)}"
            }
        )
    except Exception as e:
        error_traceback = traceback.format_exc()
        logger.error(f"âŒ [STT] TRANSCRIPTION ERROR: {str(e)}")
        logger.error(f"ðŸ“‹ [STT] Error type: {type(e).__name__}")
        logger.error(f"ðŸ“‹ [STT] Full traceback:\n{error_traceback}")
        
        # Clean up any remaining temp files
        if temp_audio_path and os.path.exists(temp_audio_path):
            os.unlink(temp_audio_path)
        if wav_path and os.path.exists(wav_path):
            os.unlink(wav_path)
            
        raise HTTPException(
            status_code=500,
            detail={
                "error": "transcription_error",
                "message": f"Transcription failed: {str(e)}"
            }
        )


@router.get("/health")
async def transcribe_health():
    """Health check for transcription service with mental health context + VAD."""
    return {
        "status": "ok",
        "service": "transcription",
        "type": "rest_api_with_mental_health_context_and_vad",
        "model": "latest_short",
        "languages": ["mr-IN", "hi-IN", "en-IN"],
        "context_phrases": len(MENTAL_HEALTH_PHRASES),
        "vad_enabled": True,
        "vad_thresholds": {
            "min_max_amplitude": 5000,
            "min_avg_amplitude": 1000
        },
        "message": "Mental health transcription service with doctor voice filtering ready"
    }
